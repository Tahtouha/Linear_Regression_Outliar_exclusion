# -*- coding: utf-8 -*-
"""ZERNADJI_TAHA_TP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fOsdQMLXZoWt758ojVbcUic1_cMLo-Pq

#ZERNADJI Taha

<h1>Linear Regression:</h1>
"""

import numpy as np
from matplotlib import pyplot as plt

"""<p>1. Generate 2000 data points with one dimensional feature randomly sampled. For each data point attribute a real number representing its target value.</p>

"""

n_samples = 2000
x =  np.random.rand(n_samples)
y =  np.random.rand(n_samples)

x1 = np.random.rand(n_samples,1)
y1 = np.random.rand(n_samples,1)

"""<p>2. Fit a line using the data points and their target values using two different methods</p>

# Method 1: Linear regression direct minimization
"""

#method 1: using Direct minimization

un= np.ones(2000).T
newX= np.column_stack((un, x.T))
teta = y.T @ newX @ np.power(newX.T @ newX, -1)
plt.plot(teta-1,"r")
plt.scatter(x,y)

"""# Method 2: Gradient decent

"""

itr=100
lr=0.35
thetaa = np.random.randn(2, 1)

for i in range(0, itr):
  thetaa = thetaa - lr * (1/n_samples * newX.T.dot((newX.dot(thetaa) - y1)))

plt.scatter(x1,y1)
plt.plot(x1,newX.dot(thetaa),color='red')
plt.legend()

"""<p>3. Add 20 outlier data points randomly distributed, set them and their target values far away from the original data point</p>

"""

n_outliers = 20
#creating 10 random values betweeb 0 and 15 for x and y and defining them as outliers
outlier_x = np.random.choice(15,n_outliers)
outlier_y = np.random.choice(15,n_outliers)

new_out_x = np.concatenate((x, outlier_x), axis=0)
new_out_y = np.concatenate((y, outlier_y), axis=0)

"""<p>4. Fit a line using all data points using one of the proposed two methods. What do you observe?</p>

"""

#Unsing the first method 
un= np.ones(2020).T
newX= np.column_stack((un, new_out_x.T))
teta = new_out_y.T @ newX @ np.power(newX.T @ newX, -1)
plt.plot(teta-1,"r")
plt.scatter(x,y)

"""# **Observation**
Now that we've added outliars, the resulting line is skewed buy a big margine, the data points we've added that have values between 0 and 15 are very far from the original data points that have values between 0 and 1, so the resulting line is skewed towards the higher values.

<p>5. Add a regularisation to the proposed method, use the sum of the square of coefficients. What do you observe?</p>
"""

alpha = 500
#We have a large alpha to pay more attention to the regularization terms, if alpha is low we end up having both lines (with and without regularizaition) being superimposed.
teta2  = np.dot(np.dot(new_out_y, newX), np.power(np.dot(newX.T,newX)+ alpha * np.identity(2),-1))
plt.plot(teta2-1, "r") #the red line represents the method with added regularization 
plt.plot(teta-1,"y") #the yellow line is the previous result with no regularization
plt.scatter(x,y)

"""<p>6. Fit linear model with RANSAC algorithm</p>"""

from sklearn.linear_model import RANSACRegressor

n_outliers = 20
X_outlier = np.append(x1,np.random.rand(n_outliers,1),axis=0)
Y_outlier = np.append(y1,np.random.uniform(low=2 ,high=4, size=(n_outliers,1)), axis=0)

ransac = RANSACRegressor()

ransac.fit(X_outlier,Y_outlier)

plt.scatter(X_outlier, Y_outlier, color='blue',label='new datasset')
plt.plot(X_outlier,ransac.predict(X_outlier), color='black', label="RANSAC algorithm")
plt.legend()